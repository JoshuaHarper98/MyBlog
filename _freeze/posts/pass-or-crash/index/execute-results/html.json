{
  "hash": "7cedbfd20377e270016b377ae1497cee",
  "result": {
    "markdown": "---\ntitle: 'Pass or Crash: Investigating the Link between Driving Test Pass Rates and Accident Numbers.'\nauthor: Joshua Harper\ndate: '2023-05-16'\ncategories:\n  - python\n  - visualisation\n  - statistics\nimage: image.jpg\nformat:\n  html:\n    code-fold: true\n---\n\nIn this data-driven investigation, we will use statistics and Python to explore the potential association between driving test pass rates and the number of road accidents.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical Analysis\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nsns.set(rc={\"figure.figsize\":(16, 8)})\n```\n:::\n\n\nThe data used in this analysis was collected from publicly available resources. The driver test centre data was collected from:\n* Scotland, England, Wales - https://www.gov.uk/government/statistical-data-sets/car-driving-test-data-by-test-centre\n* Northern Ireland - https://www.opendatani.gov.uk/dataset/driver-testing-outcomes-by-test-centre\n\nThe accident data was collected from STATS19. Further information on how to access this data can be found here: https://docs.ropensci.org/stats19/index.html\n\nAfter collecting the data, I used web scrapping to find the postcode of each driving test centre. I then used GeoPy to extract the latitude and longitude from the postcodes. I mapped the coordinates of each accident to the coordinates of the nearest driving centre using the haversine formula:\n\n$$\n  haversin\\left(\\frac{d}{r}\\right) = haversin\\left(\\phi_{1} - \\phi_{2}\\right) + cos\\left(\\phi_{1}\\right)cos\\left(\\phi_{2}\\right)haversin\\left(\\lambda_{1} - \\lambda_{2}\\right),\n$$\n\nwhere $r$ is the radius of the earth, $d$ is the distance between two points, $\\phi_{1}, \\phi_{2}$ are the latitudes of the two points and $\\lambda_{1}, \\lambda_{2}$ are the longitudes of the two points.\n\nLet's load the data and see what information we have!\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pd.read_csv('crashes_merged_2017_to_2020.csv', engine='pyarrow', encoding='ISO-8859-1')\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata.head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accident_index</th>\n      <th>accident_year</th>\n      <th>accident_severity</th>\n      <th>number_of_vehicles</th>\n      <th>number_of_casualties</th>\n      <th>nearest_test_centre</th>\n      <th>2020_passrate</th>\n      <th>2019_passrate</th>\n      <th>2018_passrate</th>\n      <th>2017_passrate</th>\n      <th>2020_conducted</th>\n      <th>2019_conducted</th>\n      <th>2018_conducted</th>\n      <th>2017_conducted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Fatal</td>\n      <td>2</td>\n      <td>3</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>2</td>\n      <td>4</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2.017010e+12</td>\n      <td>2017</td>\n      <td>Slight</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Tottenham</td>\n      <td>43.046875</td>\n      <td>39.634783</td>\n      <td>37.53915</td>\n      <td>37.65838824</td>\n      <td>1280</td>\n      <td>5750.0</td>\n      <td>4470.0</td>\n      <td>3946</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFor each accident, we have **accident_severity**: the severity of the accident - Slight, Serious or Fatal. The **number_of_vehicles** and **number_of_casualties** in each accident. The year of the accident, **accident_year**. The **nearest_test_centre** to the accident, along with the pass rates for the years **2020_passrate**, **2019_passrate**, **2018_passrate** and **2017_passrate**, along with the respective **20XX_conducted** the number of tests conducted for that year.\n\nWe will first check the data for any errors that may have been inherited from Excel.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndata.dtypes\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\naccident_index          float64\naccident_year             int64\naccident_severity        object\nnumber_of_vehicles        int64\nnumber_of_casualties      int64\nnearest_test_centre      object\n2020_passrate            object\n2019_passrate           float64\n2018_passrate           float64\n2017_passrate            object\n2020_conducted           object\n2019_conducted          float64\n2018_conducted          float64\n2017_conducted           object\ndtype: object\n```\n:::\n:::\n\n\nSome features we expect to be numeric are being loaded as objects, this is most likely due to loading missing values incorrectly.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndata = data[~(data == '').any(axis=1)]\ndata = data[~(data == '..').any(axis=1)]\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata['2020_passrate'] = data['2020_passrate'].astype('float64')\ndata['2017_passrate'] = data['2017_passrate'].astype('float64')\ndata['2020_conducted'] = data['2020_conducted'].astype('float64')\ndata['2017_conducted'] = data['2017_conducted'].astype('float64')\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndata.isna().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\naccident_index          52858\naccident_year               0\naccident_severity           0\nnumber_of_vehicles          0\nnumber_of_casualties        0\nnearest_test_centre         0\n2020_passrate               0\n2019_passrate               0\n2018_passrate               0\n2017_passrate               0\n2020_conducted              0\n2019_conducted              0\n2018_conducted              0\n2017_conducted              0\ndtype: int64\n```\n:::\n:::\n\n\nThe data now looks to be correct. Next, we will plot the average driving test pass rate and the average number of accidents over the observed period.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True)\nsns.lineplot(data = (data\n                     .groupby('nearest_test_centre')\n                     .mean()[['2020_passrate', '2019_passrate', '2018_passrate', '2017_passrate']]\n                     .rename(columns={'2017_passrate':2017, '2018_passrate':2018, '2019_passrate':2019, '2020_passrate':2020})\n                     .melt(var_name='Year', value_name='Pass Rate', ignore_index=False)\n                     .reset_index(drop=False)),\n             x = 'Year',\n             y = 'Pass Rate',\n             ax=ax1\n             )\nsns.lineplot(data = data.groupby('accident_year')['accident_index'].count(), ax=ax2)\nax1.title.set_text('Average driving test pass rate')\nax2.title.set_text('Average number of accidents')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=1258 height=677}\n:::\n:::\n\n\nFrom the first plot, The pass rates of driving tests from 2017 to 2019 remained relatively stable, ranging from 48.88% to 49.33%. However, there was a noticeable increase in the pass rate to 53.23% in 2020. Similarly, the second plot shows a general decreasing trend in accident rates between 2017 and 2019, with a larger decrease 2020. This spike in pass rates and a significant drop in accident rates could be due to various reasons, such as improved driving habits, improved road safety measures, or even the impact of the COVID-19 pandemic on testing and driving patterns. It is not conclusive at this stage if there is a direct association between the two results.\n\nWe will now create a new feature, **passrate**, that combines the four pass rate columns.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndata['passrate'] = (data[['2017_passrate', '2018_passrate','2019_passrate','2020_passrate']]\n                     .values[np.where(data['accident_year']\n                                      .values.reshape(-1,1) == [2017,2018,2019,2020])])\n```\n:::\n\n\nGrouping the data by **nearest_driving_test_centre** and **accident_year**.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n```\n:::\n\n\nWe can now plot the distribution of the driving test pass rates, highlighting the year the test took place.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nsns.histplot(data = grouped_df, x='passrate', hue='accident_year')\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<AxesSubplot:xlabel='passrate', ylabel='Count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=1258 height=657}\n:::\n:::\n\n\nThe histogram reinforces the conclusions seen earlier, the pass rates for 2017 to 2019 have a roughly similar shape. Whereas the pass rates for 2020 are more right-skewed. The mean pass rate over all 4 years is **50.0976998216524%**\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nsns.histplot(data = grouped_df, x='number_of_accidents', hue='accident_year')\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n<AxesSubplot:xlabel='number_of_accidents', ylabel='Count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-2.png){width=1258 height=657}\n:::\n:::\n\n\nThe histogram shows that the number of accidents per year is highly right-skewed for all 4 years. The mean number of accidents per year is **373.5821917808219**. There appear to be many test centres with 0 or close to 0 accidents a year. To investigate this further, we will look at the test centres with extreme values for both the number of accidents and the pass rates.\n\nLooking first at test centres with the lowest number of accidents and the highest pass rates.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ngrouped_df.sort_values(['number_of_accidents', 'passrate'], ascending = [True, False]).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>accident_year</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>219</th>\n      <td>Buckie</td>\n      <td>2020</td>\n      <td>54.545455</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>Campbeltown</td>\n      <td>2020</td>\n      <td>76.470588</td>\n      <td>7</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>Campbeltown</td>\n      <td>2019</td>\n      <td>73.684211</td>\n      <td>7</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>217</th>\n      <td>Buckie</td>\n      <td>2018</td>\n      <td>57.094595</td>\n      <td>7</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>Lochgilphead</td>\n      <td>2020</td>\n      <td>74.074074</td>\n      <td>8</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>479</th>\n      <td>Golspie</td>\n      <td>2020</td>\n      <td>73.333333</td>\n      <td>8</td>\n      <td>6</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1019</th>\n      <td>Stranraer</td>\n      <td>2020</td>\n      <td>67.500000</td>\n      <td>8</td>\n      <td>5</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1015</th>\n      <td>Stornoway</td>\n      <td>2020</td>\n      <td>65.591398</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>459</th>\n      <td>Girvan</td>\n      <td>2020</td>\n      <td>63.636364</td>\n      <td>8</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>375</th>\n      <td>Dunoon</td>\n      <td>2020</td>\n      <td>62.745098</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nUnsurprisingly, the test centres with the lowest number of accidents are in areas that have low population sizes. The pass rates do appear to be significantly higher than the average for the dataset.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ngrouped_df.sort_values(['passrate', 'number_of_accidents'], ascending = [False, True]).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>accident_year</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>443</th>\n      <td>Fraserburgh</td>\n      <td>2020</td>\n      <td>82.456140</td>\n      <td>11</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>679</th>\n      <td>Lerwick</td>\n      <td>2020</td>\n      <td>80.303030</td>\n      <td>11</td>\n      <td>7</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>Duns</td>\n      <td>2020</td>\n      <td>80.000000</td>\n      <td>13</td>\n      <td>3</td>\n      <td>8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>Kingussie</td>\n      <td>2020</td>\n      <td>78.571429</td>\n      <td>13</td>\n      <td>7</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>701</th>\n      <td>Lochgilphead</td>\n      <td>2018</td>\n      <td>77.181208</td>\n      <td>22</td>\n      <td>15</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>476</th>\n      <td>Golspie</td>\n      <td>2017</td>\n      <td>76.712329</td>\n      <td>10</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>Campbeltown</td>\n      <td>2020</td>\n      <td>76.470588</td>\n      <td>7</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>703</th>\n      <td>Lochgilphead</td>\n      <td>2020</td>\n      <td>74.074074</td>\n      <td>8</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>Campbeltown</td>\n      <td>2019</td>\n      <td>73.684211</td>\n      <td>7</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>479</th>\n      <td>Golspie</td>\n      <td>2020</td>\n      <td>73.333333</td>\n      <td>8</td>\n      <td>6</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAgain, we can see that the high pass rate test centres have a low number of accidents. Now looking at the test centres with the highest number of accidents and the lowest pass rate\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ngrouped_df.sort_values(['number_of_accidents', 'passrate'], ascending = [False, True]).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>accident_year</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>572</th>\n      <td>Hither Green (London)</td>\n      <td>2017</td>\n      <td>46.643311</td>\n      <td>4306</td>\n      <td>3665</td>\n      <td>617</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>1140</th>\n      <td>Wood Green (London)</td>\n      <td>2017</td>\n      <td>38.481506</td>\n      <td>4297</td>\n      <td>3647</td>\n      <td>634</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>573</th>\n      <td>Hither Green (London)</td>\n      <td>2018</td>\n      <td>48.732903</td>\n      <td>4194</td>\n      <td>3529</td>\n      <td>648</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>574</th>\n      <td>Hither Green (London)</td>\n      <td>2019</td>\n      <td>48.579034</td>\n      <td>4097</td>\n      <td>3490</td>\n      <td>598</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>Wood Green (London)</td>\n      <td>2018</td>\n      <td>39.565325</td>\n      <td>3802</td>\n      <td>3164</td>\n      <td>629</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1142</th>\n      <td>Wood Green (London)</td>\n      <td>2019</td>\n      <td>41.515348</td>\n      <td>3765</td>\n      <td>3162</td>\n      <td>586</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>Hither Green (London)</td>\n      <td>2020</td>\n      <td>45.445577</td>\n      <td>3183</td>\n      <td>2703</td>\n      <td>472</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1143</th>\n      <td>Wood Green (London)</td>\n      <td>2020</td>\n      <td>43.337881</td>\n      <td>2804</td>\n      <td>2406</td>\n      <td>387</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>612</th>\n      <td>Isleworth (Fleming Way)</td>\n      <td>2017</td>\n      <td>42.422118</td>\n      <td>1932</td>\n      <td>1659</td>\n      <td>263</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>Isleworth (Fleming Way)</td>\n      <td>2018</td>\n      <td>43.733762</td>\n      <td>1879</td>\n      <td>1545</td>\n      <td>326</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFrom this, we notice a trend with the test centres. They all appear to be in London, the most densely populated area in the UK. This highlights a fundamental issue of the data, the number of accidents is not normalised by population size.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ngrouped_df.sort_values(['passrate', 'number_of_accidents'], ascending = [True, False]).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>accident_year</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>415</th>\n      <td>Erith (London)</td>\n      <td>2020</td>\n      <td>29.806530</td>\n      <td>162</td>\n      <td>136</td>\n      <td>26</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>Erith (London)</td>\n      <td>2017</td>\n      <td>30.580880</td>\n      <td>212</td>\n      <td>178</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>Rochdale (Manchester)</td>\n      <td>2019</td>\n      <td>30.779165</td>\n      <td>277</td>\n      <td>210</td>\n      <td>61</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>Erith (London)</td>\n      <td>2019</td>\n      <td>31.618544</td>\n      <td>211</td>\n      <td>175</td>\n      <td>35</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>Birmingham (South Yardley)</td>\n      <td>2019</td>\n      <td>31.693198</td>\n      <td>890</td>\n      <td>762</td>\n      <td>127</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>Belvedere (London)</td>\n      <td>2020</td>\n      <td>32.560386</td>\n      <td>472</td>\n      <td>405</td>\n      <td>66</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>Birmingham (South Yardley)</td>\n      <td>2018</td>\n      <td>32.768549</td>\n      <td>884</td>\n      <td>766</td>\n      <td>113</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>Belvedere (London)</td>\n      <td>2017</td>\n      <td>32.875458</td>\n      <td>599</td>\n      <td>531</td>\n      <td>66</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>Erith (London)</td>\n      <td>2018</td>\n      <td>33.023107</td>\n      <td>227</td>\n      <td>185</td>\n      <td>38</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>Belvedere (London)</td>\n      <td>2018</td>\n      <td>33.225749</td>\n      <td>518</td>\n      <td>442</td>\n      <td>73</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis shows that the lower pass rate test centres do have a higher number of accidents, but again we must highlight the test centres appear to be in highly populated areas.\n\nWe will now plot the pass rate against the number of accidents to investigate if there are any obvious trends.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nsns.scatterplot(data = grouped_df, x='passrate', y='number_of_accidents', hue='accident_year')\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n<AxesSubplot:xlabel='passrate', ylabel='number_of_accidents'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-2.png){width=1275 height=657}\n:::\n:::\n\n\nThe scatter plot shows evidence of a weak relationship between pass rate and the number of accidents, specifically, we can see that the test centres with low pass rates tend to have a higher number of accidents. We can also see that there appears to be greater volatility in the number of accidents for the lower pass rate test centres. There also appear to be some noticeable outliers, test centres that have a high number of accidents.\n\nTo further investigate the relationship, we will calculate the correlation matrix.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nmask = np.triu(np.ones_like(grouped_df.corr(method='kendall'), dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(grouped_df.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n<AxesSubplot:>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-2.png){width=935 height=819}\n:::\n:::\n\n\nThis matrix shows that there is evidence of a weak to moderate negative relationship between the pass rate and the number of accidents. This relationship is also consistent over the accident types, with the number of fatal accidents having the lowest correlation with pass rate. Earlier we proposed that the trend of pass rates increasing and accident numbers decreasing in 2020 was due to the driving restrictions in place during the Covid-19 pandemic. Does this hidden effect add uncertainty to these values? We will remove the year 2020 from the dataset and recalculate the correlation matrix.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nmask = np.triu(np.ones_like(grouped_df[grouped_df['accident_year'] < 2020].corr(method='kendall'), dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(grouped_df[grouped_df['accident_year'] < 2020].corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n<AxesSubplot:>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-2.png){width=935 height=819}\n:::\n:::\n\n\nThis matrix shows that the relationship is consistent before the pandemic.\n\nWe will now attempt to fit a linear regression model to investigate the relationship further.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nsns.regplot(x = 'passrate', y = 'number_of_accidents', data=grouped_df)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n<AxesSubplot:xlabel='passrate', ylabel='number_of_accidents'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-2.png){width=1275 height=657}\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nmodel_1 = smf.ols(formula='number_of_accidents ~ passrate + accident_year', data=grouped_df).fit()\nprint(model_1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     number_of_accidents   R-squared:                       0.151\nModel:                             OLS   Adj. R-squared:                  0.149\nMethod:                  Least Squares   F-statistic:                     103.4\nDate:                 Wed, 17 May 2023   Prob (F-statistic):           4.66e-42\nTime:                         00:17:53   Log-Likelihood:                -8568.3\nNo. Observations:                 1168   AIC:                         1.714e+04\nDf Residuals:                     1165   BIC:                         1.716e+04\nDf Model:                            2                                         \nCovariance Type:             nonrobust                                         \n=================================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept      3.657e+04   1.99e+04      1.839      0.066   -2439.465    7.56e+04\npassrate        -18.3997      1.333    -13.806      0.000     -21.014     -15.785\naccident_year   -17.4747      9.855     -1.773      0.076     -36.810       1.861\n==============================================================================\nOmnibus:                     1334.802   Durbin-Watson:                   0.555\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           120015.268\nSkew:                           5.696   Prob(JB):                         0.00\nKurtosis:                      51.335   Cond. No.                     3.69e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.69e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\nThe adjusted R-squared value of 0.149 indicates that the model explains only 14.9% of the variance in the number_of_accidents.\n\nThe F-statistic of 103.4 and a p-value of 4.66e-42 indicate that the model as a whole is significant. This means that at least one of the independent variables in the model is significant in explaining the number_of_accidents.\n\nFor the individual independent variables, passrate has a t-statistic of -13.806 and a p-value of 0.000, indicating that it is significant in explaining the number_of_accidents. The accident_year variable has a t-statistic of -1.773 and a p-value of 0.076, indicating that it is not significant in explaining the number_of_accidents at the 5% significance level.\n\nThe coefficients of the regression indicate that for a 1 percent increase in passrate, the number_of_accidents decreases by 13.806 on average.\n\nThe Omnibus test has a p-value of 0.000, indicating that the residuals are not normally distributed. The Skew and Kurtosis values are also large and positive, indicating that the residuals are heavily skewed to the right and have a peaked distribution.\n\nThe condition number of 3.69e+06 is large, which indicates the presence of multicollinearity or other numerical problems. This could lead to unstable and unreliable regression coefficients and suggests that caution should be taken when interpreting the results.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\npred_ols = model_1.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(grouped_df['nearest_test_centre'], grouped_df['number_of_accidents'], \"o\", label=\"data\")\nax.plot(grouped_df['nearest_test_centre'], model_1.fittedvalues, \"r--.\", label=\"OLS\")\nax.plot(grouped_df['nearest_test_centre'], iv_u, \"r--\")\nax.plot(grouped_df['nearest_test_centre'], iv_l, \"r--\")\nax.legend(loc=\"best\")\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n<matplotlib.legend.Legend at 0x1f802f52d40>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-2.png){width=1264 height=637}\n:::\n:::\n\n\nThe plot of the observed values versus the fitted values with the confidence interval included shows that, for most test centres, the model fits the number_of_accidents between the confidence interval. There are several outliters, with test_centres have higher accident numbers than fitted. We will use an influence plot to investigate some of the outliers.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nfig = sm.graphics.influence_plot(model_1, criterion=\"cooks\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-1.png){width=1264 height=692}\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ngrouped_df[grouped_df['nearest_test_centre'] == grouped_df.iloc[572]['nearest_test_centre']]\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>accident_year</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>572</th>\n      <td>Hither Green (London)</td>\n      <td>2017</td>\n      <td>46.643311</td>\n      <td>4306</td>\n      <td>3665</td>\n      <td>617</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>573</th>\n      <td>Hither Green (London)</td>\n      <td>2018</td>\n      <td>48.732903</td>\n      <td>4194</td>\n      <td>3529</td>\n      <td>648</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>574</th>\n      <td>Hither Green (London)</td>\n      <td>2019</td>\n      <td>48.579034</td>\n      <td>4097</td>\n      <td>3490</td>\n      <td>598</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>Hither Green (London)</td>\n      <td>2020</td>\n      <td>45.445577</td>\n      <td>3183</td>\n      <td>2703</td>\n      <td>472</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe Hither Green test centre accounts for outlier 572, 573, 574, and 575. We can see that the test centre has a below average pass rate and an extremely high number of accidents.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ngrouped_df[grouped_df['nearest_test_centre'] == grouped_df.iloc[443]['nearest_test_centre']]\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>accident_year</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>440</th>\n      <td>Fraserburgh</td>\n      <td>2017</td>\n      <td>63.576159</td>\n      <td>20</td>\n      <td>14</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>441</th>\n      <td>Fraserburgh</td>\n      <td>2018</td>\n      <td>60.251046</td>\n      <td>21</td>\n      <td>14</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>442</th>\n      <td>Fraserburgh</td>\n      <td>2019</td>\n      <td>66.115702</td>\n      <td>19</td>\n      <td>11</td>\n      <td>7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>443</th>\n      <td>Fraserburgh</td>\n      <td>2020</td>\n      <td>82.456140</td>\n      <td>11</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe outlier 443 is the Fraserburgh test centre in 2020. We can see in that year there was a massive improvement in pass rate and just over half the number of accidents compared to prevous years.\n\nAs accident_year was noted to be statistically insignificant, we will run the analysis again without the accident_year feature. We will first create a new dataset, grouping by only nearest_driving_test_centre. This means for each driving test centre, we will have the total number of accidents over the four years.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n```\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ngrouped_2_df = (grouped_df\n                .groupby(['nearest_test_centre',]).agg(\n                    passrate=('passrate', np.mean),\n                    number_of_accidents=('number_of_accidents', np.mean),\n                    number_of_slight_accidents=('number_of_slight_accidents', np.mean),\n                    number_of_serious_accidents=('number_of_serious_accidents', np.mean),\n                    number_of_fatal_accidents=('number_of_fatal_accidents', np.mean))\n              .reset_index(drop=False)\n              )\n```\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ngrouped_2_df\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Aberdeen North</td>\n      <td>54.854631</td>\n      <td>105.75</td>\n      <td>70.00</td>\n      <td>34.25</td>\n      <td>1.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Aberdeen South (Cove)</td>\n      <td>57.237618</td>\n      <td>57.25</td>\n      <td>35.25</td>\n      <td>20.25</td>\n      <td>1.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abergavenny</td>\n      <td>58.753888</td>\n      <td>110.50</td>\n      <td>74.25</td>\n      <td>32.00</td>\n      <td>4.25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aberystwyth (Park Avenue)</td>\n      <td>53.623044</td>\n      <td>120.00</td>\n      <td>91.75</td>\n      <td>25.75</td>\n      <td>2.50</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Airdrie</td>\n      <td>47.398923</td>\n      <td>192.50</td>\n      <td>148.75</td>\n      <td>40.75</td>\n      <td>3.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>Workington</td>\n      <td>53.100607</td>\n      <td>284.75</td>\n      <td>221.75</td>\n      <td>58.25</td>\n      <td>4.75</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>Worksop</td>\n      <td>55.829321</td>\n      <td>297.75</td>\n      <td>216.75</td>\n      <td>73.50</td>\n      <td>7.50</td>\n    </tr>\n    <tr>\n      <th>289</th>\n      <td>Wrexham</td>\n      <td>42.650556</td>\n      <td>218.25</td>\n      <td>161.50</td>\n      <td>53.50</td>\n      <td>3.25</td>\n    </tr>\n    <tr>\n      <th>290</th>\n      <td>Yeovil</td>\n      <td>62.411467</td>\n      <td>348.00</td>\n      <td>289.00</td>\n      <td>53.25</td>\n      <td>5.75</td>\n    </tr>\n    <tr>\n      <th>291</th>\n      <td>York</td>\n      <td>52.422700</td>\n      <td>441.75</td>\n      <td>371.00</td>\n      <td>63.75</td>\n      <td>7.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>292 rows × 6 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nmodel_2 = smf.ols(formula='number_of_accidents ~ passrate', data=grouped_2_df).fit()\nprint(model_2.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     number_of_accidents   R-squared:                       0.158\nModel:                             OLS   Adj. R-squared:                  0.155\nMethod:                  Least Squares   F-statistic:                     54.24\nDate:                 Wed, 17 May 2023   Prob (F-statistic):           1.85e-12\nTime:                         00:18:05   Log-Likelihood:                -2136.4\nNo. Observations:                  292   AIC:                             4277.\nDf Residuals:                      290   BIC:                             4284.\nDf Model:                            1                                         \nCovariance Type:             nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1399.5167    140.937      9.930      0.000    1122.128    1676.905\npassrate     -20.4787      2.781     -7.365      0.000     -25.952     -15.006\n==============================================================================\nOmnibus:                      366.583   Durbin-Watson:                   2.150\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27816.188\nSkew:                           5.653   Prob(JB):                         0.00\nKurtosis:                      49.459   Cond. No.                         334.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nThis model appears to be a slight improvement to the previous model:\n*   The adjusted R-squared has improved from 0.149 to 0.155, indicating that the model may be a better fit.\n*   The coefficient of passrate has decreased from -18.3997 to -20.4787, this suggests a stronger negative association between passrate and the number of accidents.\n*   The issues of non-normality of the residuals persists for this model.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\npred_ols = model_2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(grouped_2_df['nearest_test_centre'], grouped_2_df['number_of_accidents'], \"o\", label=\"data\")\nax.plot(grouped_2_df['nearest_test_centre'], model_2.fittedvalues, \"r--.\", label=\"OLS\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_u, \"r--\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_l, \"r--\")\nax.legend(loc=\"best\")\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n<matplotlib.legend.Legend at 0x1f80438f4c0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-31-output-2.png){width=1264 height=637}\n:::\n:::\n\n\nThe plot of the observed values versus the fitted values shows a similar number of outliers compared to the plot for model 1. We should note that model 2 was fit on 292 observations compared to 1168 for model 1. Therefore, we should expect a similar decrease in the number of outliers. We will now use an influence plot to investigate these outliers.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nfig = sm.graphics.influence_plot(model_2, criterion=\"cooks\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-1.png){width=1264 height=688}\n:::\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ngrouped_2_df[grouped_2_df['nearest_test_centre'] == grouped_2_df.iloc[143]['nearest_test_centre']]\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>143</th>\n      <td>Hither Green (London)</td>\n      <td>47.350206</td>\n      <td>3945.0</td>\n      <td>3346.75</td>\n      <td>583.75</td>\n      <td>14.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ngrouped_2_df[grouped_2_df['nearest_test_centre'] == grouped_2_df.iloc[94]['nearest_test_centre']]\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>passrate</th>\n      <th>number_of_accidents</th>\n      <th>number_of_slight_accidents</th>\n      <th>number_of_serious_accidents</th>\n      <th>number_of_fatal_accidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>94</th>\n      <td>Duns</td>\n      <td>72.56084</td>\n      <td>16.75</td>\n      <td>9.75</td>\n      <td>6.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe Hither Green test centre is once again an outlier with a lower than average pass rate and a very high number of accidents. Outlier 94 is the Duns test centre with an extremely high pass rate and low number of accidents.\n\nWe noted earlier that the true effect of pass rate on accident rates may be uncertain due to the number of accidents not accouting for differences in population size. Therefore, higher-population areas have a higher number of crashes. To attempt to mitigate this issue, we will use the number of driving tests conducted as a pseudo value for population size.\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\ndata['conducted'] = (data[['2017_conducted', '2018_conducted','2019_conducted','2020_conducted']]\n                     .values[np.where(data['accident_year']\n                                      .values.reshape(-1,1) == [2017,2018,2019,2020])])\n```\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  conducted=('conducted', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n```\n:::\n\n\nWe divide the number of accidents by the number of tests conducted and multiply by 1000 to get the number of accidents per 1000 tests (or people).\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\ngrouped_df['accidents_adjusted'] = grouped_df['number_of_accidents']/grouped_df['conducted'] * 1000\ngrouped_df['slight_adjusted'] = grouped_df['number_of_slight_accidents']/grouped_df['conducted'] * 1000\ngrouped_df['serious_adjusted'] = grouped_df['number_of_serious_accidents']/grouped_df['conducted'] * 1000\ngrouped_df['fatal_adjusted'] = grouped_df['number_of_fatal_accidents']/grouped_df['conducted'] * 1000\n\ngrouped_df = grouped_df[['nearest_test_centre', 'passrate', 'accidents_adjusted', 'slight_adjusted', 'serious_adjusted', 'fatal_adjusted']]\n```\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\ngrouped_2_df = (grouped_df\n                .groupby(['nearest_test_centre',]).agg(\n                    passrate=('passrate', np.mean),\n                    accidents_adjusted=('accidents_adjusted', np.mean),\n                    slight_adjusted=('slight_adjusted', np.mean),\n                    serious_adjusted=('serious_adjusted', np.mean),\n                    fatal_adjusted=('fatal_adjusted', np.mean))\n              .reset_index(drop=False)\n              )\n```\n:::\n\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\ngrouped_2_df\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>passrate</th>\n      <th>accidents_adjusted</th>\n      <th>slight_adjusted</th>\n      <th>serious_adjusted</th>\n      <th>fatal_adjusted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Aberdeen North</td>\n      <td>54.854631</td>\n      <td>39.120405</td>\n      <td>23.829155</td>\n      <td>14.723623</td>\n      <td>0.567628</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Aberdeen South (Cove)</td>\n      <td>57.237618</td>\n      <td>20.764980</td>\n      <td>12.104572</td>\n      <td>7.899692</td>\n      <td>0.760715</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abergavenny</td>\n      <td>58.753888</td>\n      <td>36.992872</td>\n      <td>24.321708</td>\n      <td>11.431776</td>\n      <td>1.239388</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aberystwyth (Park Avenue)</td>\n      <td>53.623044</td>\n      <td>169.516180</td>\n      <td>130.009211</td>\n      <td>36.264133</td>\n      <td>3.242836</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Airdrie</td>\n      <td>47.398923</td>\n      <td>74.426218</td>\n      <td>53.775133</td>\n      <td>18.387963</td>\n      <td>2.263122</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>Workington</td>\n      <td>53.100607</td>\n      <td>167.157629</td>\n      <td>126.844385</td>\n      <td>37.030612</td>\n      <td>3.282632</td>\n    </tr>\n    <tr>\n      <th>288</th>\n      <td>Worksop</td>\n      <td>55.829321</td>\n      <td>106.139175</td>\n      <td>74.833260</td>\n      <td>28.565052</td>\n      <td>2.740863</td>\n    </tr>\n    <tr>\n      <th>289</th>\n      <td>Wrexham</td>\n      <td>42.650556</td>\n      <td>77.457255</td>\n      <td>56.728892</td>\n      <td>19.529580</td>\n      <td>1.198783</td>\n    </tr>\n    <tr>\n      <th>290</th>\n      <td>Yeovil</td>\n      <td>62.411467</td>\n      <td>98.353587</td>\n      <td>81.594604</td>\n      <td>14.977450</td>\n      <td>1.781533</td>\n    </tr>\n    <tr>\n      <th>291</th>\n      <td>York</td>\n      <td>52.422700</td>\n      <td>94.550250</td>\n      <td>78.899257</td>\n      <td>13.950239</td>\n      <td>1.700754</td>\n    </tr>\n  </tbody>\n</table>\n<p>292 rows × 6 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nsns.regplot(x = 'passrate', y = 'accidents_adjusted', data=grouped_2_df)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n<AxesSubplot:xlabel='passrate', ylabel='accidents_adjusted'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-40-output-2.png){width=1275 height=657}\n:::\n:::\n\n\nThe scatterplot of pass rate and adjusted accident rates now shows evidence of a positive linear association. Note that there appears to be a fairly large outlier at around ~61% pass rate and ~1190 adjusted accidents.\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nmask = np.triu(np.ones_like(grouped_2_df.corr(method='kendall'), dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(grouped_df.corr(method='kendall'), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\n<AxesSubplot:>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-41-output-2.png){width=753 height=637}\n:::\n:::\n\n\nThe correlation matrix also shows a positive association between pass rate and the adjusted accident rates. The strongest association is between pass rate and fatal accidents.\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\nmodel_3 = smf.ols(formula='accidents_adjusted ~ passrate', data=grouped_2_df).fit()\nprint(model_3.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:     accidents_adjusted   R-squared:                       0.013\nModel:                            OLS   Adj. R-squared:                  0.009\nMethod:                 Least Squares   F-statistic:                     3.751\nDate:                Wed, 17 May 2023   Prob (F-statistic):             0.0537\nTime:                        00:18:18   Log-Likelihood:                -1760.9\nNo. Observations:                 292   AIC:                             3526.\nDf Residuals:                     290   BIC:                             3533.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     48.0936     38.948      1.235      0.218     -28.563     124.750\npassrate       1.4883      0.768      1.937      0.054      -0.024       3.001\n==============================================================================\nOmnibus:                      345.971   Durbin-Watson:                   2.023\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            21976.918\nSkew:                           5.171   Prob(JB):                         0.00\nKurtosis:                      44.223   Cond. No.                         334.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nSummary of the model:\n*   The adjusted R-squared of 0.013 is significantly lower than the previous two models, indicating that this model is a worst fit to the data.\n*   The coefficient of passrate id positive, indicating a positive association between the pass rate and adjusted accident rates. This is the opposite of the relationship shown in the first model.\n*   The p-value for passrate is not significant at a significance level of 0.05, unlike the previous two models.\n*   The high skewness and kurtosis values also suggest a non-normal distribution of residuals. This could indicate that there might be other variables or non-linear relationships not captured by the model. This is consistent with the previous two models.\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\npred_ols = model_3.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(grouped_2_df['nearest_test_centre'], grouped_2_df['accidents_adjusted'], \"o\", label=\"data\")\nax.plot(grouped_2_df['nearest_test_centre'], model_3.fittedvalues, \"r--.\", label=\"OLS\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_u, \"r--\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_l, \"r--\")\nax.legend(loc=\"best\")\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n<matplotlib.legend.Legend at 0x1f8016c5e10>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-43-output-2.png){width=1255 height=637}\n:::\n:::\n\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nfig = sm.graphics.influence_plot(model_3, criterion=\"cooks\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-44-output-1.png){width=1264 height=698}\n:::\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\ngrouped_2_df[grouped_2_df['nearest_test_centre'] == grouped_2_df.iloc[275]['nearest_test_centre']]\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nearest_test_centre</th>\n      <th>passrate</th>\n      <th>accidents_adjusted</th>\n      <th>slight_adjusted</th>\n      <th>serious_adjusted</th>\n      <th>fatal_adjusted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>275</th>\n      <td>Wellingborough</td>\n      <td>61.390541</td>\n      <td>1165.93911</td>\n      <td>855.6676</td>\n      <td>275.021914</td>\n      <td>35.249596</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe influence plot below highlights that the most influential outlier is the Wellingborough test centre. This test centre has an above average pass rate but extremely high number of accidents.\n\nAfter normalising for population size there no longer appears to be any evidence of an association between driving test pass rates and accident rates. We can show that the main driver of the effect shown in the first model was actually most likely the effect of differences in population size rather than the pass rate of the test centre.\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  conducted=('conducted', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(16, 8))\n\nsns.scatterplot(data = grouped_df, x='conducted', y='passrate', hue='accident_year', ax=ax1)\nsns.scatterplot(data = grouped_df, x='conducted', y='number_of_accidents', hue='accident_year', ax=ax2)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-46-output-1.png){width=1273 height=657}\n:::\n:::\n\n\nThe first plot shows that as the number of test conducted increases; the pass rate decreases. The second plot shows that as the number of tests conducted increases; the number of accidents increases. This shows the source of the negative association between pass rate and the number of accidents.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}