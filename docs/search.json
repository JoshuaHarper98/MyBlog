[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Pass or Crash: Investigating the Link between Driving Test Pass Rates and Accident Numbers.\n\n\n\n\n\n\n\npython\n\n\nvisualisation\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nJoshua Harper\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nJoshua Harper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pass-or-crash/index.html",
    "href": "posts/pass-or-crash/index.html",
    "title": "Pass or Crash: Investigating the Link between Driving Test Pass Rates and Accident Numbers.",
    "section": "",
    "text": "In this data-driven investigation, we will use statistics and Python to explore the potential association between driving test pass rates and the number of road accidents.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical Analysis\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nsns.set(rc={\"figure.figsize\":(16, 8)})\n\n\nThe data used in this analysis was collected from publicly available resources. The driver test centre data was collected from: * Scotland, England, Wales - https://www.gov.uk/government/statistical-data-sets/car-driving-test-data-by-test-centre * Northern Ireland - https://www.opendatani.gov.uk/dataset/driver-testing-outcomes-by-test-centre\nThe accident data was collected from STATS19. Further information on how to access this data can be found here: https://docs.ropensci.org/stats19/index.html\nAfter collecting the data, I used web scrapping to find the postcode of each driving test centre. I then used GeoPy to extract the latitude and longitude from the postcodes. I mapped the coordinates of each accident to the coordinates of the nearest driving centre using the haversine formula:\n\\[\n  haversin\\left(\\frac{d}{r}\\right) = haversin\\left(\\phi_{1} - \\phi_{2}\\right) + cos\\left(\\phi_{1}\\right)cos\\left(\\phi_{2}\\right)haversin\\left(\\lambda_{1} - \\lambda_{2}\\right),\n\\]\nwhere \\(r\\) is the radius of the earth, \\(d\\) is the distance between two points, \\(\\phi_{1}, \\phi_{2}\\) are the latitudes of the two points and \\(\\lambda_{1}, \\lambda_{2}\\) are the longitudes of the two points.\nLetâ€™s load the data and see what information we have!\n\n\nCode\ndata = pd.read_csv('crashes_merged_2017_to_2020.csv', engine='pyarrow', encoding='ISO-8859-1')\n\n\n\n\nCode\ndata.head(10)\n\n\n\n\n\n\n\n\n\naccident_index\naccident_year\naccident_severity\nnumber_of_vehicles\nnumber_of_casualties\nnearest_test_centre\n2020_passrate\n2019_passrate\n2018_passrate\n2017_passrate\n2020_conducted\n2019_conducted\n2018_conducted\n2017_conducted\n\n\n\n\n0\n2.017010e+12\n2017\nFatal\n2\n3\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n1\n2.017010e+12\n2017\nSlight\n2\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n2\n2.017010e+12\n2017\nSlight\n2\n4\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n3\n2.017010e+12\n2017\nSlight\n2\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n4\n2.017010e+12\n2017\nSlight\n2\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n5\n2.017010e+12\n2017\nSlight\n2\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n6\n2.017010e+12\n2017\nSlight\n1\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n7\n2.017010e+12\n2017\nSlight\n2\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n8\n2.017010e+12\n2017\nSlight\n1\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n9\n2.017010e+12\n2017\nSlight\n2\n1\nTottenham\n43.046875\n39.634783\n37.53915\n37.65838824\n1280\n5750.0\n4470.0\n3946\n\n\n\n\n\n\n\nFor each accident, we have accident_severity: the severity of the accident - Slight, Serious or Fatal. The number_of_vehicles and number_of_casualties in each accident. The year of the accident, accident_year. The nearest_test_centre to the accident, along with the pass rates for the years 2020_passrate, 2019_passrate, 2018_passrate and 2017_passrate, along with the respective 20XX_conducted the number of tests conducted for that year.\nWe will first check the data for any errors that may have been inherited from Excel.\n\n\nCode\ndata.dtypes\n\n\naccident_index          float64\naccident_year             int64\naccident_severity        object\nnumber_of_vehicles        int64\nnumber_of_casualties      int64\nnearest_test_centre      object\n2020_passrate            object\n2019_passrate           float64\n2018_passrate           float64\n2017_passrate            object\n2020_conducted           object\n2019_conducted          float64\n2018_conducted          float64\n2017_conducted           object\ndtype: object\n\n\nSome features we expect to be numeric are being loaded as objects, this is most likely due to loading missing values incorrectly.\n\n\nCode\ndata = data[~(data == '').any(axis=1)]\ndata = data[~(data == '..').any(axis=1)]\n\n\n\n\nCode\ndata['2020_passrate'] = data['2020_passrate'].astype('float64')\ndata['2017_passrate'] = data['2017_passrate'].astype('float64')\ndata['2020_conducted'] = data['2020_conducted'].astype('float64')\ndata['2017_conducted'] = data['2017_conducted'].astype('float64')\n\n\n\n\nCode\ndata.isna().sum()\n\n\naccident_index          52858\naccident_year               0\naccident_severity           0\nnumber_of_vehicles          0\nnumber_of_casualties        0\nnearest_test_centre         0\n2020_passrate               0\n2019_passrate               0\n2018_passrate               0\n2017_passrate               0\n2020_conducted              0\n2019_conducted              0\n2018_conducted              0\n2017_conducted              0\ndtype: int64\n\n\nThe data now looks to be correct. Next, we will plot the average driving test pass rate and the average number of accidents over the observed period.\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True)\nsns.lineplot(data = (data\n                     .groupby('nearest_test_centre')\n                     .mean()[['2020_passrate', '2019_passrate', '2018_passrate', '2017_passrate']]\n                     .rename(columns={'2017_passrate':2017, '2018_passrate':2018, '2019_passrate':2019, '2020_passrate':2020})\n                     .melt(var_name='Year', value_name='Pass Rate', ignore_index=False)\n                     .reset_index(drop=False)),\n             x = 'Year',\n             y = 'Pass Rate',\n             ax=ax1\n             )\nsns.lineplot(data = data.groupby('accident_year')['accident_index'].count(), ax=ax2)\nax1.title.set_text('Average driving test pass rate')\nax2.title.set_text('Average number of accidents')\n\n\n\n\n\nFrom the first plot, The pass rates of driving tests from 2017 to 2019 remained relatively stable, ranging from 48.88% to 49.33%. However, there was a noticeable increase in the pass rate to 53.23% in 2020. Similarly, the second plot shows a general decreasing trend in accident rates between 2017 and 2019, with a larger decrease 2020. This spike in pass rates and a significant drop in accident rates could be due to various reasons, such as improved driving habits, improved road safety measures, or even the impact of the COVID-19 pandemic on testing and driving patterns. It is not conclusive at this stage if there is a direct association between the two results.\nWe will now create a new feature, passrate, that combines the four pass rate columns.\n\n\nCode\ndata['passrate'] = (data[['2017_passrate', '2018_passrate','2019_passrate','2020_passrate']]\n                     .values[np.where(data['accident_year']\n                                      .values.reshape(-1,1) == [2017,2018,2019,2020])])\n\n\nGrouping the data by nearest_driving_test_centre and accident_year.\n\n\nCode\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n\n\nWe can now plot the distribution of the driving test pass rates, highlighting the year the test took place.\n\n\nCode\nsns.histplot(data = grouped_df, x='passrate', hue='accident_year')\n\n\n&lt;AxesSubplot:xlabel='passrate', ylabel='Count'&gt;\n\n\n\n\n\nThe histogram reinforces the conclusions seen earlier, the pass rates for 2017 to 2019 have a roughly similar shape. Whereas the pass rates for 2020 are more right-skewed. The mean pass rate over all 4 years is 50.0976998216524%\n\n\nCode\nsns.histplot(data = grouped_df, x='number_of_accidents', hue='accident_year')\n\n\n&lt;AxesSubplot:xlabel='number_of_accidents', ylabel='Count'&gt;\n\n\n\n\n\nThe histogram shows that the number of accidents per year is highly right-skewed for all 4 years. The mean number of accidents per year is 373.5821917808219. There appear to be many test centres with 0 or close to 0 accidents a year. To investigate this further, we will look at the test centres with extreme values for both the number of accidents and the pass rates.\nLooking first at test centres with the lowest number of accidents and the highest pass rates.\n\n\nCode\ngrouped_df.sort_values(['number_of_accidents', 'passrate'], ascending = [True, False]).head(10)\n\n\n\n\n\n\n\n\n\nnearest_test_centre\naccident_year\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n219\nBuckie\n2020\n54.545455\n4\n1\n2\n1\n\n\n247\nCampbeltown\n2020\n76.470588\n7\n4\n2\n1\n\n\n246\nCampbeltown\n2019\n73.684211\n7\n4\n3\n0\n\n\n217\nBuckie\n2018\n57.094595\n7\n2\n3\n2\n\n\n703\nLochgilphead\n2020\n74.074074\n8\n4\n2\n2\n\n\n479\nGolspie\n2020\n73.333333\n8\n6\n2\n0\n\n\n1019\nStranraer\n2020\n67.500000\n8\n5\n3\n0\n\n\n1015\nStornoway\n2020\n65.591398\n8\n5\n2\n1\n\n\n459\nGirvan\n2020\n63.636364\n8\n4\n4\n0\n\n\n375\nDunoon\n2020\n62.745098\n8\n5\n2\n1\n\n\n\n\n\n\n\nUnsurprisingly, the test centres with the lowest number of accidents are in areas that have low population sizes. The pass rates do appear to be significantly higher than the average for the dataset.\n\n\nCode\ngrouped_df.sort_values(['passrate', 'number_of_accidents'], ascending = [False, True]).head(10)\n\n\n\n\n\n\n\n\n\nnearest_test_centre\naccident_year\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n443\nFraserburgh\n2020\n82.456140\n11\n5\n6\n0\n\n\n679\nLerwick\n2020\n80.303030\n11\n7\n4\n0\n\n\n379\nDuns\n2020\n80.000000\n13\n3\n8\n2\n\n\n635\nKingussie\n2020\n78.571429\n13\n7\n4\n2\n\n\n701\nLochgilphead\n2018\n77.181208\n22\n15\n6\n1\n\n\n476\nGolspie\n2017\n76.712329\n10\n8\n1\n1\n\n\n247\nCampbeltown\n2020\n76.470588\n7\n4\n2\n1\n\n\n703\nLochgilphead\n2020\n74.074074\n8\n4\n2\n2\n\n\n246\nCampbeltown\n2019\n73.684211\n7\n4\n3\n0\n\n\n479\nGolspie\n2020\n73.333333\n8\n6\n2\n0\n\n\n\n\n\n\n\nAgain, we can see that the high pass rate test centres have a low number of accidents. Now looking at the test centres with the highest number of accidents and the lowest pass rate\n\n\nCode\ngrouped_df.sort_values(['number_of_accidents', 'passrate'], ascending = [False, True]).head(10)\n\n\n\n\n\n\n\n\n\nnearest_test_centre\naccident_year\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n572\nHither Green (London)\n2017\n46.643311\n4306\n3665\n617\n24\n\n\n1140\nWood Green (London)\n2017\n38.481506\n4297\n3647\n634\n16\n\n\n573\nHither Green (London)\n2018\n48.732903\n4194\n3529\n648\n17\n\n\n574\nHither Green (London)\n2019\n48.579034\n4097\n3490\n598\n9\n\n\n1141\nWood Green (London)\n2018\n39.565325\n3802\n3164\n629\n9\n\n\n1142\nWood Green (London)\n2019\n41.515348\n3765\n3162\n586\n17\n\n\n575\nHither Green (London)\n2020\n45.445577\n3183\n2703\n472\n8\n\n\n1143\nWood Green (London)\n2020\n43.337881\n2804\n2406\n387\n11\n\n\n612\nIsleworth (Fleming Way)\n2017\n42.422118\n1932\n1659\n263\n10\n\n\n613\nIsleworth (Fleming Way)\n2018\n43.733762\n1879\n1545\n326\n8\n\n\n\n\n\n\n\nFrom this, we notice a trend with the test centres. They all appear to be in London, the most densely populated area in the UK. This highlights a fundamental issue of the data, the number of accidents is not normalised by population size.\n\n\nCode\ngrouped_df.sort_values(['passrate', 'number_of_accidents'], ascending = [True, False]).head(10)\n\n\n\n\n\n\n\n\n\nnearest_test_centre\naccident_year\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n415\nErith (London)\n2020\n29.806530\n162\n136\n26\n0\n\n\n412\nErith (London)\n2017\n30.580880\n212\n178\n33\n1\n\n\n898\nRochdale (Manchester)\n2019\n30.779165\n277\n210\n61\n6\n\n\n414\nErith (London)\n2019\n31.618544\n211\n175\n35\n1\n\n\n130\nBirmingham (South Yardley)\n2019\n31.693198\n890\n762\n127\n1\n\n\n107\nBelvedere (London)\n2020\n32.560386\n472\n405\n66\n1\n\n\n129\nBirmingham (South Yardley)\n2018\n32.768549\n884\n766\n113\n5\n\n\n104\nBelvedere (London)\n2017\n32.875458\n599\n531\n66\n2\n\n\n413\nErith (London)\n2018\n33.023107\n227\n185\n38\n4\n\n\n105\nBelvedere (London)\n2018\n33.225749\n518\n442\n73\n3\n\n\n\n\n\n\n\nThis shows that the lower pass rate test centres do have a higher number of accidents, but again we must highlight the test centres appear to be in highly populated areas.\nWe will now plot the pass rate against the number of accidents to investigate if there are any obvious trends.\n\n\nCode\nsns.scatterplot(data = grouped_df, x='passrate', y='number_of_accidents', hue='accident_year')\n\n\n&lt;AxesSubplot:xlabel='passrate', ylabel='number_of_accidents'&gt;\n\n\n\n\n\nThe scatter plot shows evidence of a weak relationship between pass rate and the number of accidents, specifically, we can see that the test centres with low pass rates tend to have a higher number of accidents. We can also see that there appears to be greater volatility in the number of accidents for the lower pass rate test centres. There also appear to be some noticeable outliers, test centres that have a high number of accidents.\nTo further investigate the relationship, we will calculate the correlation matrix.\n\n\nCode\nmask = np.triu(np.ones_like(grouped_df.corr(method='kendall'), dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(grouped_df.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nThis matrix shows that there is evidence of a weak to moderate negative relationship between the pass rate and the number of accidents. This relationship is also consistent over the accident types, with the number of fatal accidents having the lowest correlation with pass rate. Earlier we proposed that the trend of pass rates increasing and accident numbers decreasing in 2020 was due to the driving restrictions in place during the Covid-19 pandemic. Does this hidden effect add uncertainty to these values? We will remove the year 2020 from the dataset and recalculate the correlation matrix.\n\n\nCode\nmask = np.triu(np.ones_like(grouped_df[grouped_df['accident_year'] &lt; 2020].corr(method='kendall'), dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(grouped_df[grouped_df['accident_year'] &lt; 2020].corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nThis matrix shows that the relationship is consistent before the pandemic.\nWe will now attempt to fit a linear regression model to investigate the relationship further.\n\n\nCode\nsns.regplot(x = 'passrate', y = 'number_of_accidents', data=grouped_df)\n\n\n&lt;AxesSubplot:xlabel='passrate', ylabel='number_of_accidents'&gt;\n\n\n\n\n\n\n\nCode\nmodel_1 = smf.ols(formula='number_of_accidents ~ passrate + accident_year', data=grouped_df).fit()\nprint(model_1.summary())\n\n\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     number_of_accidents   R-squared:                       0.151\nModel:                             OLS   Adj. R-squared:                  0.149\nMethod:                  Least Squares   F-statistic:                     103.4\nDate:                 Wed, 17 May 2023   Prob (F-statistic):           4.66e-42\nTime:                         00:17:53   Log-Likelihood:                -8568.3\nNo. Observations:                 1168   AIC:                         1.714e+04\nDf Residuals:                     1165   BIC:                         1.716e+04\nDf Model:                            2                                         \nCovariance Type:             nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept      3.657e+04   1.99e+04      1.839      0.066   -2439.465    7.56e+04\npassrate        -18.3997      1.333    -13.806      0.000     -21.014     -15.785\naccident_year   -17.4747      9.855     -1.773      0.076     -36.810       1.861\n==============================================================================\nOmnibus:                     1334.802   Durbin-Watson:                   0.555\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           120015.268\nSkew:                           5.696   Prob(JB):                         0.00\nKurtosis:                      51.335   Cond. No.                     3.69e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.69e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe adjusted R-squared value of 0.149 indicates that the model explains only 14.9% of the variance in the number_of_accidents.\nThe F-statistic of 103.4 and a p-value of 4.66e-42 indicate that the model as a whole is significant. This means that at least one of the independent variables in the model is significant in explaining the number_of_accidents.\nFor the individual independent variables, passrate has a t-statistic of -13.806 and a p-value of 0.000, indicating that it is significant in explaining the number_of_accidents. The accident_year variable has a t-statistic of -1.773 and a p-value of 0.076, indicating that it is not significant in explaining the number_of_accidents at the 5% significance level.\nThe coefficients of the regression indicate that for a 1 percent increase in passrate, the number_of_accidents decreases by 13.806 on average.\nThe Omnibus test has a p-value of 0.000, indicating that the residuals are not normally distributed. The Skew and Kurtosis values are also large and positive, indicating that the residuals are heavily skewed to the right and have a peaked distribution.\nThe condition number of 3.69e+06 is large, which indicates the presence of multicollinearity or other numerical problems. This could lead to unstable and unreliable regression coefficients and suggests that caution should be taken when interpreting the results.\n\n\nCode\npred_ols = model_1.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(grouped_df['nearest_test_centre'], grouped_df['number_of_accidents'], \"o\", label=\"data\")\nax.plot(grouped_df['nearest_test_centre'], model_1.fittedvalues, \"r--.\", label=\"OLS\")\nax.plot(grouped_df['nearest_test_centre'], iv_u, \"r--\")\nax.plot(grouped_df['nearest_test_centre'], iv_l, \"r--\")\nax.legend(loc=\"best\")\n\n\n&lt;matplotlib.legend.Legend at 0x1f802f52d40&gt;\n\n\n\n\n\nThe plot of the observed values versus the fitted values with the confidence interval included shows that, for most test centres, the model fits the number_of_accidents between the confidence interval. There are several outliters, with test_centres have higher accident numbers than fitted. We will use an influence plot to investigate some of the outliers.\n\n\nCode\nfig = sm.graphics.influence_plot(model_1, criterion=\"cooks\")\n\n\n\n\n\n\n\nCode\ngrouped_df[grouped_df['nearest_test_centre'] == grouped_df.iloc[572]['nearest_test_centre']]\n\n\n\n\n\n\n\n\n\nnearest_test_centre\naccident_year\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n572\nHither Green (London)\n2017\n46.643311\n4306\n3665\n617\n24\n\n\n573\nHither Green (London)\n2018\n48.732903\n4194\n3529\n648\n17\n\n\n574\nHither Green (London)\n2019\n48.579034\n4097\n3490\n598\n9\n\n\n575\nHither Green (London)\n2020\n45.445577\n3183\n2703\n472\n8\n\n\n\n\n\n\n\nThe Hither Green test centre accounts for outlier 572, 573, 574, and 575. We can see that the test centre has a below average pass rate and an extremely high number of accidents.\n\n\nCode\ngrouped_df[grouped_df['nearest_test_centre'] == grouped_df.iloc[443]['nearest_test_centre']]\n\n\n\n\n\n\n\n\n\nnearest_test_centre\naccident_year\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n440\nFraserburgh\n2017\n63.576159\n20\n14\n6\n0\n\n\n441\nFraserburgh\n2018\n60.251046\n21\n14\n6\n1\n\n\n442\nFraserburgh\n2019\n66.115702\n19\n11\n7\n1\n\n\n443\nFraserburgh\n2020\n82.456140\n11\n5\n6\n0\n\n\n\n\n\n\n\nThe outlier 443 is the Fraserburgh test centre in 2020. We can see in that year there was a massive improvement in pass rate and just over half the number of accidents compared to prevous years.\nAs accident_year was noted to be statistically insignificant, we will run the analysis again without the accident_year feature. We will first create a new dataset, grouping by only nearest_driving_test_centre. This means for each driving test centre, we will have the total number of accidents over the four years.\n\n\nCode\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n\n\n\n\nCode\ngrouped_2_df = (grouped_df\n                .groupby(['nearest_test_centre',]).agg(\n                    passrate=('passrate', np.mean),\n                    number_of_accidents=('number_of_accidents', np.mean),\n                    number_of_slight_accidents=('number_of_slight_accidents', np.mean),\n                    number_of_serious_accidents=('number_of_serious_accidents', np.mean),\n                    number_of_fatal_accidents=('number_of_fatal_accidents', np.mean))\n              .reset_index(drop=False)\n              )\n\n\n\n\nCode\ngrouped_2_df\n\n\n\n\n\n\n\n\n\nnearest_test_centre\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n0\nAberdeen North\n54.854631\n105.75\n70.00\n34.25\n1.50\n\n\n1\nAberdeen South (Cove)\n57.237618\n57.25\n35.25\n20.25\n1.75\n\n\n2\nAbergavenny\n58.753888\n110.50\n74.25\n32.00\n4.25\n\n\n3\nAberystwyth (Park Avenue)\n53.623044\n120.00\n91.75\n25.75\n2.50\n\n\n4\nAirdrie\n47.398923\n192.50\n148.75\n40.75\n3.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n287\nWorkington\n53.100607\n284.75\n221.75\n58.25\n4.75\n\n\n288\nWorksop\n55.829321\n297.75\n216.75\n73.50\n7.50\n\n\n289\nWrexham\n42.650556\n218.25\n161.50\n53.50\n3.25\n\n\n290\nYeovil\n62.411467\n348.00\n289.00\n53.25\n5.75\n\n\n291\nYork\n52.422700\n441.75\n371.00\n63.75\n7.00\n\n\n\n\n292 rows Ã— 6 columns\n\n\n\n\n\nCode\nmodel_2 = smf.ols(formula='number_of_accidents ~ passrate', data=grouped_2_df).fit()\nprint(model_2.summary())\n\n\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:     number_of_accidents   R-squared:                       0.158\nModel:                             OLS   Adj. R-squared:                  0.155\nMethod:                  Least Squares   F-statistic:                     54.24\nDate:                 Wed, 17 May 2023   Prob (F-statistic):           1.85e-12\nTime:                         00:18:05   Log-Likelihood:                -2136.4\nNo. Observations:                  292   AIC:                             4277.\nDf Residuals:                      290   BIC:                             4284.\nDf Model:                            1                                         \nCovariance Type:             nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1399.5167    140.937      9.930      0.000    1122.128    1676.905\npassrate     -20.4787      2.781     -7.365      0.000     -25.952     -15.006\n==============================================================================\nOmnibus:                      366.583   Durbin-Watson:                   2.150\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            27816.188\nSkew:                           5.653   Prob(JB):                         0.00\nKurtosis:                      49.459   Cond. No.                         334.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThis model appears to be a slight improvement to the previous model: * The adjusted R-squared has improved from 0.149 to 0.155, indicating that the model may be a better fit. * The coefficient of passrate has decreased from -18.3997 to -20.4787, this suggests a stronger negative association between passrate and the number of accidents. * The issues of non-normality of the residuals persists for this model.\n\n\nCode\npred_ols = model_2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(grouped_2_df['nearest_test_centre'], grouped_2_df['number_of_accidents'], \"o\", label=\"data\")\nax.plot(grouped_2_df['nearest_test_centre'], model_2.fittedvalues, \"r--.\", label=\"OLS\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_u, \"r--\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_l, \"r--\")\nax.legend(loc=\"best\")\n\n\n&lt;matplotlib.legend.Legend at 0x1f80438f4c0&gt;\n\n\n\n\n\nThe plot of the observed values versus the fitted values shows a similar number of outliers compared to the plot for model 1. We should note that model 2 was fit on 292 observations compared to 1168 for model 1. Therefore, we should expect a similar decrease in the number of outliers. We will now use an influence plot to investigate these outliers.\n\n\nCode\nfig = sm.graphics.influence_plot(model_2, criterion=\"cooks\")\n\n\n\n\n\n\n\nCode\ngrouped_2_df[grouped_2_df['nearest_test_centre'] == grouped_2_df.iloc[143]['nearest_test_centre']]\n\n\n\n\n\n\n\n\n\nnearest_test_centre\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n143\nHither Green (London)\n47.350206\n3945.0\n3346.75\n583.75\n14.5\n\n\n\n\n\n\n\n\n\nCode\ngrouped_2_df[grouped_2_df['nearest_test_centre'] == grouped_2_df.iloc[94]['nearest_test_centre']]\n\n\n\n\n\n\n\n\n\nnearest_test_centre\npassrate\nnumber_of_accidents\nnumber_of_slight_accidents\nnumber_of_serious_accidents\nnumber_of_fatal_accidents\n\n\n\n\n94\nDuns\n72.56084\n16.75\n9.75\n6.0\n1.0\n\n\n\n\n\n\n\nThe Hither Green test centre is once again an outlier with a lower than average pass rate and a very high number of accidents. Outlier 94 is the Duns test centre with an extremely high pass rate and low number of accidents.\nWe noted earlier that the true effect of pass rate on accident rates may be uncertain due to the number of accidents not accouting for differences in population size. Therefore, higher-population areas have a higher number of crashes. To attempt to mitigate this issue, we will use the number of driving tests conducted as a pseudo value for population size.\n\n\nCode\ndata['conducted'] = (data[['2017_conducted', '2018_conducted','2019_conducted','2020_conducted']]\n                     .values[np.where(data['accident_year']\n                                      .values.reshape(-1,1) == [2017,2018,2019,2020])])\n\n\n\n\nCode\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  conducted=('conducted', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n\n\nWe divide the number of accidents by the number of tests conducted and multiply by 1000 to get the number of accidents per 1000 tests (or people).\n\n\nCode\ngrouped_df['accidents_adjusted'] = grouped_df['number_of_accidents']/grouped_df['conducted'] * 1000\ngrouped_df['slight_adjusted'] = grouped_df['number_of_slight_accidents']/grouped_df['conducted'] * 1000\ngrouped_df['serious_adjusted'] = grouped_df['number_of_serious_accidents']/grouped_df['conducted'] * 1000\ngrouped_df['fatal_adjusted'] = grouped_df['number_of_fatal_accidents']/grouped_df['conducted'] * 1000\n\ngrouped_df = grouped_df[['nearest_test_centre', 'passrate', 'accidents_adjusted', 'slight_adjusted', 'serious_adjusted', 'fatal_adjusted']]\n\n\n\n\nCode\ngrouped_2_df = (grouped_df\n                .groupby(['nearest_test_centre',]).agg(\n                    passrate=('passrate', np.mean),\n                    accidents_adjusted=('accidents_adjusted', np.mean),\n                    slight_adjusted=('slight_adjusted', np.mean),\n                    serious_adjusted=('serious_adjusted', np.mean),\n                    fatal_adjusted=('fatal_adjusted', np.mean))\n              .reset_index(drop=False)\n              )\n\n\n\n\nCode\ngrouped_2_df\n\n\n\n\n\n\n\n\n\nnearest_test_centre\npassrate\naccidents_adjusted\nslight_adjusted\nserious_adjusted\nfatal_adjusted\n\n\n\n\n0\nAberdeen North\n54.854631\n39.120405\n23.829155\n14.723623\n0.567628\n\n\n1\nAberdeen South (Cove)\n57.237618\n20.764980\n12.104572\n7.899692\n0.760715\n\n\n2\nAbergavenny\n58.753888\n36.992872\n24.321708\n11.431776\n1.239388\n\n\n3\nAberystwyth (Park Avenue)\n53.623044\n169.516180\n130.009211\n36.264133\n3.242836\n\n\n4\nAirdrie\n47.398923\n74.426218\n53.775133\n18.387963\n2.263122\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n287\nWorkington\n53.100607\n167.157629\n126.844385\n37.030612\n3.282632\n\n\n288\nWorksop\n55.829321\n106.139175\n74.833260\n28.565052\n2.740863\n\n\n289\nWrexham\n42.650556\n77.457255\n56.728892\n19.529580\n1.198783\n\n\n290\nYeovil\n62.411467\n98.353587\n81.594604\n14.977450\n1.781533\n\n\n291\nYork\n52.422700\n94.550250\n78.899257\n13.950239\n1.700754\n\n\n\n\n292 rows Ã— 6 columns\n\n\n\n\n\nCode\nsns.regplot(x = 'passrate', y = 'accidents_adjusted', data=grouped_2_df)\n\n\n&lt;AxesSubplot:xlabel='passrate', ylabel='accidents_adjusted'&gt;\n\n\n\n\n\nThe scatterplot of pass rate and adjusted accident rates now shows evidence of a positive linear association. Note that there appears to be a fairly large outlier at around ~61% pass rate and ~1190 adjusted accidents.\n\n\nCode\nmask = np.triu(np.ones_like(grouped_2_df.corr(method='kendall'), dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(grouped_df.corr(method='kendall'), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nThe correlation matrix also shows a positive association between pass rate and the adjusted accident rates. The strongest association is between pass rate and fatal accidents.\n\n\nCode\nmodel_3 = smf.ols(formula='accidents_adjusted ~ passrate', data=grouped_2_df).fit()\nprint(model_3.summary())\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:     accidents_adjusted   R-squared:                       0.013\nModel:                            OLS   Adj. R-squared:                  0.009\nMethod:                 Least Squares   F-statistic:                     3.751\nDate:                Wed, 17 May 2023   Prob (F-statistic):             0.0537\nTime:                        00:18:18   Log-Likelihood:                -1760.9\nNo. Observations:                 292   AIC:                             3526.\nDf Residuals:                     290   BIC:                             3533.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     48.0936     38.948      1.235      0.218     -28.563     124.750\npassrate       1.4883      0.768      1.937      0.054      -0.024       3.001\n==============================================================================\nOmnibus:                      345.971   Durbin-Watson:                   2.023\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            21976.918\nSkew:                           5.171   Prob(JB):                         0.00\nKurtosis:                      44.223   Cond. No.                         334.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nSummary of the model: * The adjusted R-squared of 0.013 is significantly lower than the previous two models, indicating that this model is a worst fit to the data. * The coefficient of passrate id positive, indicating a positive association between the pass rate and adjusted accident rates. This is the opposite of the relationship shown in the first model. * The p-value for passrate is not significant at a significance level of 0.05, unlike the previous two models. * The high skewness and kurtosis values also suggest a non-normal distribution of residuals. This could indicate that there might be other variables or non-linear relationships not captured by the model. This is consistent with the previous two models.\n\n\nCode\npred_ols = model_3.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(grouped_2_df['nearest_test_centre'], grouped_2_df['accidents_adjusted'], \"o\", label=\"data\")\nax.plot(grouped_2_df['nearest_test_centre'], model_3.fittedvalues, \"r--.\", label=\"OLS\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_u, \"r--\")\nax.plot(grouped_2_df['nearest_test_centre'], iv_l, \"r--\")\nax.legend(loc=\"best\")\n\n\n&lt;matplotlib.legend.Legend at 0x1f8016c5e10&gt;\n\n\n\n\n\n\n\nCode\nfig = sm.graphics.influence_plot(model_3, criterion=\"cooks\")\n\n\n\n\n\n\n\nCode\ngrouped_2_df[grouped_2_df['nearest_test_centre'] == grouped_2_df.iloc[275]['nearest_test_centre']]\n\n\n\n\n\n\n\n\n\nnearest_test_centre\npassrate\naccidents_adjusted\nslight_adjusted\nserious_adjusted\nfatal_adjusted\n\n\n\n\n275\nWellingborough\n61.390541\n1165.93911\n855.6676\n275.021914\n35.249596\n\n\n\n\n\n\n\nThe influence plot below highlights that the most influential outlier is the Wellingborough test centre. This test centre has an above average pass rate but extremely high number of accidents.\nAfter normalising for population size there no longer appears to be any evidence of an association between driving test pass rates and accident rates. We can show that the main driver of the effect shown in the first model was actually most likely the effect of differences in population size rather than the pass rate of the test centre.\n\n\nCode\ngrouped_df = (data\n              .assign(slight_accident = data['accident_severity'] == 'Slight')\n              .assign(serious_accident = data['accident_severity'] == 'Serious')\n              .assign(fatal_accident = data['accident_severity'] == 'Fatal')\n              .groupby(['nearest_test_centre', 'accident_year']).agg(\n                  passrate=('passrate', np.mean),\n                  conducted=('conducted', np.mean),\n                  number_of_accidents=('nearest_test_centre', 'count'),\n                  number_of_slight_accidents=('slight_accident', np.sum),\n                  number_of_serious_accidents=('serious_accident', np.sum),\n                  number_of_fatal_accidents=('fatal_accident', np.sum))\n              .reset_index(drop=False)\n              )\n\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(16, 8))\n\nsns.scatterplot(data = grouped_df, x='conducted', y='passrate', hue='accident_year', ax=ax1)\nsns.scatterplot(data = grouped_df, x='conducted', y='number_of_accidents', hue='accident_year', ax=ax2)\n\nplt.show()\n\n\n\n\n\nThe first plot shows that as the number of test conducted increases; the pass rate decreases. The second plot shows that as the number of tests conducted increases; the number of accidents increases. This shows the source of the negative association between pass rate and the number of accidents."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]